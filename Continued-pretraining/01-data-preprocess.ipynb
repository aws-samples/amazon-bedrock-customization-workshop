{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data before running the customization job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We recommend to run this notebook using \n",
    ">- kernel: `Data Science 3.0` or `Python 3`\n",
    ">- instance size `ml.t3.medium` or greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is based on a PDF file containing 1,625 European CapMarkets and Bank Finance terms.\n",
    "Let's first download the dataset from [Latham & Watkins' website](https://www.lw.com/en/book-of-jargon/boj-european-capital-markets-and-bank-finance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('capmarkets-jargon.pdf', <http.client.HTTPMessage at 0x103d34c10>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://www.lw.com/admin/Upload/Documents/Books%20of%20Jargon/Book-of-Jargon-European-Capital-Markets-and-Bank-Finance-2nd-Edition.pdf\"\n",
    "filename = \"capmarkets-jargon\"\n",
    "filename_pdf = \"capmarkets-jargon.pdf\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is in PDF format, we need to install PyPDF2 library to be able to extract text from PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /Applications/miniconda3/lib/python3.10/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a function that converts each line of a PDF page into JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_counter = 0\n",
    "parts = []\n",
    "\n",
    "### Construct JSONL from one PDF page\n",
    "\n",
    "def visitor_body(text, cm, tm, fontDict, fontSize):\n",
    "    global terms_counter\n",
    "    \n",
    "    if len(text) > 1:\n",
    "        if(fontDict['/BaseFont'] == '/IYOEDH+CandidaStd-Bold'):\n",
    "            # close previous term, if there's one\n",
    "            if(parts):\n",
    "                parts.append(\"\\\"}\")\n",
    "                parts.append(\"\\n\")\n",
    "\n",
    "\n",
    "            terms_counter = terms_counter + 1\n",
    "            \n",
    "            parts.append(\"{\\\"input\\\":\" + \" \\\"\" + text )\n",
    "        else:\n",
    "            parts.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iterate through each PDF page containing definitions converting them to JSONL and then save the output as capmarkets-jargon.jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms processed: 1624\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdfReader = PdfReader(filename_pdf) \n",
    "start_page = 3\n",
    "end_page = 186\n",
    "terms_counter = 0\n",
    "\n",
    "# clear all old items, if any\n",
    "parts.clear()\n",
    "output = \"\"\n",
    "for i in range(start_page, end_page):\n",
    "    page = pdfReader.pages[i]\n",
    "    page.extract_text(visitor_text=visitor_body)\n",
    "\n",
    "# close the last term\n",
    "parts.append( \"\\\"\" + \"}\" )\n",
    "\n",
    "print(\"Terms processed:\", terms_counter)\n",
    "\n",
    "output = \"\".join(parts)\n",
    "with open(filename + \".jsonl\", \"w\") as outfile:\n",
    "    outfile.write(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's upload the generated JSONL file to S3 so that it's accessible by Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded capmarkets-jargon.jsonl to s3://sagemaker-us-east-1-922650977840/PreProcessed/capmarkets-jargon.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the training data\n",
    "train_data_path = filename + \".jsonl\"\n",
    "\n",
    "# Upload the training data to the specified S3 key prefix 'PreProcessed'\n",
    "s3_train_data = sagemaker_session.upload_data(path=train_data_path, key_prefix='PreProcessed')\n",
    "\n",
    "# Print a message indicating the successful upload\n",
    "print(f\"Uploaded {train_data_path} to {s3_train_data}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
